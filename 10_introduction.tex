% !TeX root = 00_paper_main.tex

\section{Introduction}

%%% Part 2: who is the addressee?
Modern \textsc{Knowledge Base Management Systems} allow the ingestion of various data representations, such as relational, graph and full-text data \cite{ibmwatson}. Current KBMS usually ingest reliable data sources, such as encyclopaedic data \cite{ibmwatson}, business data \cite{Saha16} and medical journals and clinical data \cite{WANG201834}. 
%%% Part 1: what I want to say precisely? Which is the broader problem that I want to solve?
Despite such KBMS are able to reconcile different representation towards an uniform one \cite{Niu}, no technique is currently exploited for detecting \textit{contradicting} facts for hypothesis generation: in particular both data collected from on-line social network \cite{Lazer1094} or even medical diagnoses \cite{imihl} may contain contradictions.


\begin{example}
\textit{One of the simplest ways to find contradictions is to check whether there are facts that are both affirmed and denied at the same time. E.g., fact ``Casu Marzu is not a good cheese'' is a rebuttal of the fact ``Casu Marzu is a exquisite cheese''. Alternatively, we can focus on factual representations that do not allow the contemporaneity of two alternative hypotheses, thus violating a functional dependency. E.g., fact ``Yesterday Alice flew to Berlin'' contradicts ``Yesterday Alice took a trip to Kearney'' because Alice cannot be found in two different places at the same time.}
\end{example}

As previously stated, the inherent inconsistency of spurious data sources leads to the generation of conflicting hypotheses in response to a query. The inability of detecting such inconsistencies prohibits to weight how many KBMS facts either support or discard a given hypothesis, thus preventing from correctly ranking the generated hypotheses. This whole scenario demands for a query language which interpretation is able to detect such inconsistencies from the data, and an hypothesis generation task that is able to generate hypotheses which do not contain contradictions. 

%%% Part 4: communicate the Idea that we have (what is an inconsistency)
In order to solve this practical problem, we focus on querying domain-specific knowledge bases containing the aforementioned form of inconsistencies. Such setting allows to use ontologies to represent knowledge, thus allowing to represent both structured (tables), semistructured (wikis, web pages) and unstructured (full texts, videos, images) documents with a uniform representation \cite{DeepDivePhD,Chen14}. MeSH\footnote{\url{https://www.nlm.nih.gov/mesh/}} is an example of an ontology for medical settings. Ontologies prompt the intended semantics and properties of such final representations: they differentiate \textit{entities} from \textit{fillers} (e.g., properties), where the formers are individual units that can be described by different qualities represented as the latter. They also distinguish the \textit{(binary) relationships} involving such entities, and the \textit{facts} providing a description of the observed world, which may involve multiple entities and fillers. Events are a specific case of facts. As previously mentioned, this ideal data description does not prevent the data to be biased: as an example, a different point of view may provide biased factual descriptions, that need to be detected. In order to do so, taxonomies and semantic networks focus more on describing entities and fillers through \textit{relationships}, thus allowing to detect data inconsistencies \cite{Lependu11}.  ICD-11\footnote{\url{https://icd.who.int/}} is a well-known medical taxonomy used to identify and categorize specific diseases and medical procedures uniquely. Ontologies and taxonomies provide the pillars sustaining the HypoGator framework, which consists of a pipeline composed of the following blocks, which are described in more detail in the homonymous paper sections (\S):
	

\begin{itemize}
\item \textbf{Query Interpretation} (\S\ref{sec:querinterp}): any generic query (SPARQL, CYPHER, SQL) is represented as a graph $q$ \cite{Hu0YWZ18} and then used to perform an approximated graph matching with the knowledge base \cite{DeVirgilio2015}.
\item \textbf{Hypothesis Generation} (\S\ref{sec:hypgen}): the previous approximated graph matching generates the preliminar hypotheses $h$: similar or coherent hypotheses are merged (``synthesis''), while conflicting hypotheses are kept separated. %Hypotheses containing inconsistencies are filtered out, while the remaining ones are represented as a graph.
\item \textbf{Hypothesis Evidence and Scoring} (\S\ref{sec:scoring}): before returning such hypotheses, we must know which knowledge base element support the provided hypotheses and which does not. Such evidence extraction is used to provide a first scoring function counting how many supporting facts validate the hypothesis against the remaining ones (either supporting or discarding the hypothesis).
\item \textbf{Hypothesis Query Answering} (\S\ref{sec:synthansw}): after the previous filter phase, each hypothesis $h$ is possibly rewritten into an hypothesis $h'$ maximizing the similarity with $q$. Finally, we rank the hypotheses by combining the previous section's ranking and the edit distance between $h$ and $h'$.
\end{itemize} 


%As a result, this paper provides a formal definition of information inconsistencies, which is later on exploited to define such ranking metric; given a type of facts $t$ associated with a functional dependency $f$, we say that facts $c$ and $\tilde{c}$ of type $t$ are contradictory either if $\tilde{c}$ is the negation of $c$ or the coexistence of $c$ and $\tilde{c}$ in $t$ violates the functional dependency $f$. In order to meet our goal, we represent both events and relationships as multidimensional facts of different types $t$ via common-sense knowledge integration \cite{SpeerCH16}; to each type $t$ we associate a functional dependency by exploiting the ontologies' ability to describe a domain knowledge of interest \cite{FORTINEAU2015573}.

Before providing details of such phases, we analyse the state of the art of query interpretation (\S\ref{sec:queryanswer}), information extraction (\S\ref{sec:rulemining}-\ref{sec:matching}) and knowledge base construction (\S\ref{sec:kbconstr}).

%To improve efficiency and accuracy, we provide some link and rule mining techniques that {\color{red} [TODO]}. To summarize, we make the following contributions:
%\begin{itemize}
%	\item Section description here.
%\end{itemize}
